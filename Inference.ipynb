{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Layers import *\n",
    "from Conv2D import Conv2D, train, predict, test\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from Loss_funcs import *\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "## flatten the data from 2D to 1D (vector) as the data shape is [28x28]\n",
    "#\n",
    "# # x_train = x_train.reshape(60000, 784)\n",
    "# # x_test = x_test.reshape(10000, 784)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "#\n",
    "# ## convert to float [0.0 - 1.0]\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "# print(x_train.shape, 'train samples')\n",
    "# print(x_test.shape, 'test samples')\n",
    "#\n",
    "#\n",
    "# # One-hot encoding\n",
    "# # y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "# # y_test = tensorflow.keras.utils.to_categorical(y_test, 10)\n",
    "#\n",
    "# y_train = pd.get_dummies(y_train, dtype=float)\n",
    "# y_test = pd.get_dummies(y_test, dtype=float)\n",
    "#\n",
    "# y_train = np.asarray(y_train)\n",
    "# y_test = np.asarray(y_test)\n",
    "#\n",
    "# x_train, y_train, x_test, y_test = x_train[:500], y_train[:500], x_test[:100], y_test[:100]\n",
    "# # print(x_train[0].shape, y_train[0].reshape(-1, 1).shape)\n",
    "#\n",
    "# y_train = np.expand_dims(y_train, axis=-1)\n",
    "# y_test = np.expand_dims(y_test, axis =-1)\n",
    "#\n",
    "# print(y_train.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utils import preprocess_data_conv\n",
    "\n",
    "x_train, y_train = preprocess_data_conv(x_train, y_train, 200)\n",
    "x_test, y_test = preprocess_data_conv(x_test, y_test, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(400, 1, 28, 28)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "network = [\n",
    "    #Convolutional((1, 28, 28), (3, 3), 5, padding = (1,1)),\n",
    "    Conv2D(image_shape = (1, 28, 28), num_filters = 1, filter_size = 3, stride = (1, 1), padding_type = 'same'),\n",
    "    ReLULayer(),\n",
    "    # no padding, so shape changes\n",
    "    #Reshape (filters, depth, height, width)\n",
    "    # Reshape((5, 1, 28, 28), (5 * 1 * 28 * 28, 1)),\n",
    "    FlattenLayer(),\n",
    "    Dense(1 * 1 *28 * 28, 10),\n",
    "    SigmoidLayer(),\n",
    "    Dense(10, 2),\n",
    "    #ReLU()\n",
    "    SoftmaxLayer()\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbinary_cross_entropy_prime\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\n\u001B[0;32m      9\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DLF_Konwolucja\\Conv2D.py:167\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(network, loss, loss_prime, x_train, y_train, epochs, learning_rate, info)\u001B[0m\n\u001B[0;32m    165\u001B[0m grad \u001B[38;5;241m=\u001B[39m loss_prime(y, output)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(network):\n\u001B[1;32m--> 167\u001B[0m     grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m Sample \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(x_train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror\u001B[38;5;241m/\u001B[39mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\DLF_Konwolucja\\Conv2D.py:125\u001B[0m, in \u001B[0;36mConv2D.backward\u001B[1;34m(self, output_gradient, learning_rate)\u001B[0m\n\u001B[0;32m    120\u001B[0m     kernels_gradient[i][j] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvolve2d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput[j], np\u001B[38;5;241m.\u001B[39mrot90(output_gradient[i][j]), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m#TODO: Błąd jest tutaj, gradient kerneli zawsze wynosi zero.\u001B[39;00m\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;66;03m# kernels_gradient[i][j] = self.convolve2d(self.input[j], np.rot90(output_gradient[i]), self.stride,\u001B[39;00m\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;66;03m#                           'valid')\u001B[39;00m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;66;03m# input_gradient[j] += self.convolve2d(output_gradient[i][j], self.kernels[i][j], self.stride, 'full')\u001B[39;00m\n\u001B[1;32m--> 125\u001B[0m     input_gradient[j] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvolve2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_gradient\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernels\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfull\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernels \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m kernels_gradient\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m output_gradient\n",
      "File \u001B[1;32m~\\DLF_Konwolucja\\Conv2D.py:41\u001B[0m, in \u001B[0;36mConv2D.convolve2d\u001B[1;34m(self, image, filter, stride, padding_type)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvolve2d\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, \u001B[38;5;28mfilter\u001B[39m, stride, padding_type):\n\u001B[0;32m     39\u001B[0m \n\u001B[0;32m     40\u001B[0m     \u001B[38;5;66;03m# 28x28 no padding\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m     height, width \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;66;03m#filter_height, filter_width = filter.shape\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m padding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    network,\n",
    "    binary_cross_entropy,\n",
    "    binary_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Gradienty znikają szybko"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 2302.5850847300117, accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "test(network, binary_cross_entropy, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.],\n       [1.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(network, x_test[100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All dense test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from utils import preprocess_data_dense"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data_dense(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data_dense(x_test, y_test, 200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "network_2 = [\n",
    "    Dense(784, 40),\n",
    "    ReLULayer(),\n",
    "    Dense(40, 10),\n",
    "    Sigmoid()\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\damig\\DLF_Konwolucja\\Conv2D.py:163: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(f'\\r Sample {i}/{len(x_train)}, error: {error/i}', end=\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 760/1000, error: 0.00056678652131266166"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:04,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 970/1000, error: 0.00051239978225414757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:04,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 810/1000, error: 0.00048333552199331234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:01<00:03,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 620/1000, error: 0.00044245292122311935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:02<00:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 830/1000, error: 0.00040899292696140537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 640/1000, error: 0.00038285959583963257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:03<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 800/1000, error: 0.00035962158099670313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:03<00:01,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 630/1000, error: 0.00033607722955009644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:04<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 900/1000, error: 0.00031075297034718647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:04<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample 760/1000, error: 0.00028758083842955667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epochs: 10/10, error=0.00028649376896089726956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train(\n",
    "    network_2,\n",
    "    categorical_crossentropy,\n",
    "    categorical_crossentropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    learning_rate=0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the neural network is equal to: 0.44\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for x, y in zip(x_test, y_test):\n",
    "    y_pred.append(np.argmax(predict(network_2, x)))\n",
    "    y_true.append(np.argmax(y))\n",
    "\n",
    "# accuracy score\n",
    "print(f'Accuracy of the neural network is equal to: {accuracy_score(y_true, y_pred)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}